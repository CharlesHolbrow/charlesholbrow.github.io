<html>

<head>
	<meta charset="utf8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
	<meta name="description" content="Charles' Home Page" />
	<meta name="author" content="Charles Holbrow" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<link rel="stylesheet" href="/css/hk-pyg.raw.css" />
	<link rel="stylesheet" href="/css/master.styl.css" />
	<title>Charles Holbrow - Cybernetic Media Production</title>
	<script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
	<script src="/js/video-sizing.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
	<script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true, processEnvironments:
		true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center',
		"HTML-CSS": { styles: {'.MathJax_Display': {"margin": 0}}, linebreaks: { automatic: true } } });
	</script>
</head>

<body>
	<div class="menu"><a href="/" class="menu-item">Bio </a><a href="/posts" class="menu-item">Blog </a><a href="/projects" class="menu-item">Projects </a></div>
	<div class="content">
		<div class="post">
			<div class="post-header">
				<h2 class="post-title"><a href="/project/cybernetic-media">Cybernetic Media Production</a></h2><span class="post-date">Feb 1, 2019</span></div>
			<div class="post-content">
				<p>In the early stages of my PhD, I was working on tools for creating, testing and iterating on augmented reality music videos. I called this system <strong>Cybernetic Media Production</strong>.
					Before discarding the idea, I developed a custom augmented reality framework. Here it is in action:</p>
				<iframe title="vimeo-player" src="https://player.vimeo.com/video/305900081" width="640" height="360" frameborder="0" allowfullscreen>
				</iframe>
				<p>This is how it works:</p>
				<ul>
					<li>I mounted an <a href="https://www.vive.com/us/accessory/vive-tracker/">HTC Vive &#x201C;Puck&#x201D; Tracker</a> on a DSLR camera</li>
					<li>I wrote a <a href="https://github.com/CharlesHolbrow/vive-osc-sender">utility</a> that uses Valve&#x2019;s OpenVR C++ library to access the HTC Vive tracking
						data</li>
					<li>I created a 3D scene using the open source tools in <a href="https://openframeworks.cc/">OpenFrameworks</a>.</li>
					<li>Using the tracking data from the camera to I position a virtual camera in the 3D Space</li>
					<li>Using the tracking data from the HTC Vive controller, I put virtual objects into the scene, and layer them over the live feed from the DSLR</li>
				</ul>
				<p>During development, I discovered that it is the coincidence of many small details that makes the AR illusion effective. The latency of the video feed has to
					be perfectly matched with the latency of the tracking data. The field of view of the virtual camera needs to be carefully calibrated with the &#x201C;real&#x201D;
					camera. The virtual objects need to be smooth and I found that it was important to optimize the rendering pipeline. Efficiently rendering pretty, smooth lines
					in 3D is surprisingly difficult.</p>
				<p>The idea behind <strong>Cybernetic Media Production</strong> was to procedurally create many different versions of a music video, and then A/B test the audio
					and video content the same way that big companies like Amazon and Microsoft test different versions of their websites. Microsoft was able to use A/B testing
					to <a href="https://doi.org/10.1145/2487575.2488217">increase advertising revenue by hundreds of millions of dollars</a>. At the time, I was thinking <strong>&#x201C;shouldn&#x2019;t musicians be able to do the same thing?&#x201D;</strong></p>
				<p>I believe that Cybernetic Media Production has commercial value. I also believe that some version of this practice is likely inevitable. The technology isn&#x2019;t
					quite there to make it happen yet.</p>
				<p>Initially, I intended to spend my PhD developing the technology to make Music Video A/B testing a reality. For every creative or ethical question I answered,
					another obstacle emerged. What kinds of artistic parameters should be A/B tested? Will A/B tested content really offer new creative opportunities? The major
					web companies do a good job of measuring and manipulating the public for profit, but what are the <em>creative</em> advantages to thinking about artwork the
					way that the Big Four think about web pages?</p>
				<p>With guidance from my advisor and dissertation committee, questions like these led me to reconsider. Eventually I did move my dissertation work away from what
					you might call <strong>the Amazon of music production</strong> toward the development of infrastructure to support <strong>the Wikipedia of music production</strong>.
					Ask me about this journey if your are curious &#x2013; It&#x2019;s an interesting story, and it speaks to unique characteristics within the MIT Media Lab&#x2019;s
					unconventional academic environment.</p>
				<p>However, I was able to re-purpose the my AR framework for something Joyful. In the video below, the projections are entirely created by members of the audience
					who are manipulating the 3D Vive controllers in real time.</p>
				<iframe title="vimeo-player" src="https://player.vimeo.com/video/366604775" width="640" height="360" frameborder="0" allowfullscreen>
				</iframe>
			</div>
		</div>
	</div>
</body>

</html>